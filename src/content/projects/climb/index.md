---
name: "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models"
description: "A comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities."
tags: []
people: [david, paul]
publication: {
    venue: "arXiv preprint",
    link: "https://arxiv.org/abs/2503.07667"
}
image: "./cover.png"
---

## Abstract
Recent advances in clinical AI have enabled remarkable progress across many clinical domains. However, existing benchmarks and models are primarily limited to a small set of modalities and tasks, which hinders the development of large-scale multimodal methods that can make holistic assessments of patient health and well-being. To bridge this gap, we introduce Clinical Large-Scale Integrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark unifying diverse clinical data across imaging, language, temporal, and graph modalities. CLIMB comprises 4.51 million patient samples totaling 19.01 terabytes distributed across 2D imaging, 3D video, time series, graphs, and multimodal data. Through extensive empirical evaluation, we demonstrate that multitask pretraining significantly improves performance on understudied domains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis over single-task learning. Pretraining on CLIMB also effectively improves models' generalization capability to new tasks, and strong unimodal encoder performance translates well to multimodal performance when paired with task-appropriate fusion strategies. Our findings provide a foundation for new architecture designs and pretraining strategies to advance clinical AI research.

## Links
* [Github](https://github.com/DDVD233/CLIMB?tab=readme-ov-file)